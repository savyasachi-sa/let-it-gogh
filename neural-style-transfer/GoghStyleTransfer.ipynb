{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as td\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Setup Directories\n",
    "imageDir = os.getcwd() + '/Images/'\n",
    "styleImageDir = imageDir + 'Style/'\n",
    "contentImageDir = imageDir + 'Content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/savyasachi/let-it-gogh/neural-style-transfer/Images/\n"
     ]
    }
   ],
   "source": [
    "print(imageDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## Check Device \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Build Dataset\n",
    "\n",
    "class StyleTransferDataset(td.Dataset):\n",
    "    def __init__(self, img_size=512):\n",
    "        super(StyleTransferDataset, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.style_images_dir = styleImageDir\n",
    "        self.content_images_dir = contentImageDir\n",
    "        self.files = os.listdir(self.style_images_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"StyleTransferDataset(image_size={})\". \\\n",
    "        format(self.image_size)\n",
    "    \n",
    "    ## Returns \n",
    "    ## [Style Image, Content Image, PreProcessed Style Image Tensor, PreProcessed Content Image Tensor, OptImage]\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        style_img_path = os.path.join(self.style_images_dir, self.files[idx])\n",
    "        content_img_path = os.path.join(self.content_images_dir, self.files[idx])\n",
    "        \n",
    "        ## Referencing Original paper's work\n",
    "        pre_process = transforms.Compose([transforms.Resize(self.img_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                          ])\n",
    "        \n",
    "        imgs = [Image.open(style_img_path), Image.open(content_img_path)]\n",
    "        imgs_torch = [pre_process(img) for img in imgs]\n",
    "        imgs_torch = [Variable(img.unsqueeze(0).to(device)) for img in imgs_torch]\n",
    "\n",
    "\n",
    "        style_image, content_image = imgs_torch\n",
    "#         random init\n",
    "        opt_img = Variable(torch.randn(content_image.size()).type_as(content_image.data), requires_grad=True) \n",
    "#         opt_img = Variable(content_image.data.clone(), requires_grad=True)\n",
    "\n",
    "        return imgs + [style_image, content_image, opt_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Gram matrix and Loss\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b,c,h,w = input.size()\n",
    "        F = input.view(b, c, h*w)\n",
    "        G = torch.bmm(F, F.transpose(1,2)) \n",
    "        G.div_(h*w)\n",
    "        return G\n",
    "\n",
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Layer Name to index map to easily reference features\n",
    "layer_to_index = dict(r11=1, r12=3, r21=6, r22=8, r31=11, r32=13, r33=15, r34=17, r41=20, r42=22, r43=24, r44=26,\n",
    "                      r51=29, r52=31, r53=33, r54=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Neural Net\n",
    "class StyleTransferNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleTransferNet, self).__init__()\n",
    "        \n",
    "        self.vgg = tv.models.vgg19(pretrained=True)  \n",
    "        \n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False \n",
    "            \n",
    "    ## Runs the forward method on pretrained vgg net and returns output features for the passed layer names\n",
    "    def forward(self, x, out_layers):\n",
    "        output = {}\n",
    "        output[-1] = x\n",
    "        num_of_layers = len(self.vgg.features)\n",
    "        layers = self.vgg.features\n",
    "        \n",
    "        for i in range(num_of_layers):\n",
    "            output[i] = layers[i](output[i-1])\n",
    "\n",
    "        out_layer_indices = [layer_to_index[layer] for layer in out_layers]\n",
    "        \n",
    "        return [output[key] for key in out_layer_indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Post Processing Method to get the output image from tensor \n",
    "def postProcess(tensor):\n",
    "    postProcess1 = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "                               transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
    "                                                    std=[1,1,1]),\n",
    "                               transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
    "                               ])\n",
    "    postProcess2 = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "    t = postProcess1(tensor)\n",
    "    t[t>1] = 1    \n",
    "    t[t<0] = 0\n",
    "    img = postProcess2(t)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# compute loss functions & optimization targets\n",
    "def configureLossFunctionsAndTargets(style_image, content_image, style_layers, content_layers):\n",
    "    loss_layers = style_layers + content_layers\n",
    "    loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "    loss_fns = [loss_fn.to(device) for loss_fn in loss_fns]\n",
    "\n",
    "    #compute optimization targets\n",
    "    style_targets = [GramMatrix()(A).detach() for A in net(style_image, style_layers)]\n",
    "    content_targets = [A.detach() for A in net(content_image, content_layers)]\n",
    "    targets = style_targets + content_targets\n",
    "    \n",
    "    return loss_layers, loss_fns, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Init Net & Data\n",
    "net = StyleTransferNet().to(device)\n",
    "training_data = StyleTransferDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# configure layers & weights\n",
    "style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "content_layers = ['r42']\n",
    "\n",
    "## Configure Weights\n",
    "style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
    "content_weights = [1e1]\n",
    "weights = style_weights + content_weights\n",
    "\n",
    "## Configure Experiment\n",
    "max_iter = 500\n",
    "show_iter = 10\n",
    "\n",
    "## Run for Different Pairs\n",
    "for T in training_data:\n",
    "    style_image = T[2].to(device)\n",
    "    content_image = T[3].to(device)\n",
    "    opt_img = T[4].to(device)\n",
    "    \n",
    "    optimizer = optim.LBFGS([opt_img]);\n",
    "    loss_layers, loss_fns, targets = configureLossFunctionsAndTargets(\n",
    "        style_image, content_image, style_layers, content_layers)\n",
    "    \n",
    "    ## Run Net\n",
    "    n_iter = [0]\n",
    "    while n_iter[0] <= max_iter:\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = net(opt_img, loss_layers)\n",
    "            layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
    "            loss = sum(layer_losses)\n",
    "            loss.backward()\n",
    "            n_iter[0] += 1\n",
    "            return loss\n",
    "    \n",
    "        optimizer.step(closure)\n",
    "    \n",
    "    #display result\n",
    "    out_img = postProcess(opt_img.cpu().squeeze())\n",
    "    plt.imshow(out_img)\n",
    "    # gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-23e9b14c1892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out_img' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
